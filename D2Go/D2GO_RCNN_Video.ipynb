{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dc1b71d-c0d8-4e0b-8f7a-d32f05820a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from d2go.runner import GeneralizedRCNNRunner\n",
    "from d2go.model_zoo import model_zoo\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23b308d2-d8e8-4c4d-93a3-9d8380836434",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:d2go.runner.default_runner:Initializing control pg\n"
     ]
    }
   ],
   "source": [
    "# Load the configuration like in the training script\n",
    "runner = GeneralizedRCNNRunner()\n",
    "cfg = runner.get_default_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"faster_rcnn_fbnetv3a_C4.yaml\"))\n",
    "cfg.MODEL.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7edcdf3e-4b6f-4ccc-b676-20e14bcafdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the datasets instances\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "register_coco_instances(\"fire_train\", {}, \"/home/yali/coco-annotator/forest-fires-t.json\", \"/home/yali/coco-annotator/datasets/forest-fires/train_images\")\n",
    "register_coco_instances(\"fire_val\", {}, \"/home/yali/coco-annotator/forest-fires-v.json\", \"/home/yali/coco-annotator/datasets/forest-fires/val_images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b79cdfc-9f5d-4df1-8f7a-f1d7a1a84735",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:d2go.modeling.backbone.fbnet_v2:Build FBNet using unified arch_def:\n",
      "trunk\n",
      "- {'block_op': 'conv_k3', 'block_cfg': {'out_channels': 16, 'stride': 2}, 'stage_idx': 0, 'block_idx': 0}\n",
      "- {'block_op': 'ir_k3', 'block_cfg': {'out_channels': 16, 'stride': 1, 'expansion': 1, 'less_se_channels': False}, 'stage_idx': 0, 'block_idx': 1}\n",
      "- {'block_op': 'ir_k3', 'block_cfg': {'out_channels': 16, 'stride': 1, 'expansion': 1, 'less_se_channels': False}, 'stage_idx': 0, 'block_idx': 2}\n",
      "- {'block_op': 'ir_k5', 'block_cfg': {'out_channels': 24, 'stride': 2, 'expansion': 4, 'less_se_channels': False}, 'stage_idx': 1, 'block_idx': 0}\n",
      "- {'block_op': 'ir_k5', 'block_cfg': {'out_channels': 24, 'stride': 1, 'expansion': 3, 'less_se_channels': False}, 'stage_idx': 1, 'block_idx': 1}\n",
      "- {'block_op': 'ir_k5', 'block_cfg': {'out_channels': 24, 'stride': 1, 'expansion': 3, 'less_se_channels': False}, 'stage_idx': 1, 'block_idx': 2}\n",
      "- {'block_op': 'ir_k5', 'block_cfg': {'out_channels': 24, 'stride': 1, 'expansion': 3, 'less_se_channels': False}, 'stage_idx': 1, 'block_idx': 3}\n",
      "- {'block_op': 'ir_k5_se', 'block_cfg': {'out_channels': 32, 'stride': 2, 'expansion': 4, 'less_se_channels': False}, 'stage_idx': 2, 'block_idx': 0}\n",
      "- {'block_op': 'ir_k3_se', 'block_cfg': {'out_channels': 32, 'stride': 1, 'expansion': 3, 'less_se_channels': False}, 'stage_idx': 2, 'block_idx': 1}\n",
      "- {'block_op': 'ir_k3_se', 'block_cfg': {'out_channels': 32, 'stride': 1, 'expansion': 3, 'less_se_channels': False}, 'stage_idx': 2, 'block_idx': 2}\n",
      "- {'block_op': 'ir_k3_se', 'block_cfg': {'out_channels': 32, 'stride': 1, 'expansion': 3, 'less_se_channels': False}, 'stage_idx': 2, 'block_idx': 3}\n",
      "- {'block_op': 'ir_k5', 'block_cfg': {'out_channels': 64, 'stride': 2, 'expansion': 4, 'less_se_channels': False}, 'stage_idx': 3, 'block_idx': 0}\n",
      "- {'block_op': 'ir_k3', 'block_cfg': {'out_channels': 64, 'stride': 1, 'expansion': 3, 'less_se_channels': False}, 'stage_idx': 3, 'block_idx': 1}\n",
      "- {'block_op': 'ir_k3', 'block_cfg': {'out_channels': 64, 'stride': 1, 'expansion': 3, 'less_se_channels': False}, 'stage_idx': 3, 'block_idx': 2}\n",
      "- {'block_op': 'ir_k3', 'block_cfg': {'out_channels': 64, 'stride': 1, 'expansion': 3, 'less_se_channels': False}, 'stage_idx': 3, 'block_idx': 3}\n",
      "- {'block_op': 'ir_k5_se', 'block_cfg': {'out_channels': 112, 'stride': 1, 'expansion': 4, 'less_se_channels': False}, 'stage_idx': 3, 'block_idx': 4}\n",
      "- {'block_op': 'ir_k5_se', 'block_cfg': {'out_channels': 112, 'stride': 1, 'expansion': 3, 'less_se_channels': False}, 'stage_idx': 3, 'block_idx': 5}\n",
      "- {'block_op': 'ir_k5_se', 'block_cfg': {'out_channels': 112, 'stride': 1, 'expansion': 3, 'less_se_channels': False}, 'stage_idx': 3, 'block_idx': 6}\n",
      "- {'block_op': 'ir_k5_se', 'block_cfg': {'out_channels': 112, 'stride': 1, 'expansion': 3, 'less_se_channels': False}, 'stage_idx': 3, 'block_idx': 7}\n",
      "- {'block_op': 'ir_k5_se', 'block_cfg': {'out_channels': 112, 'stride': 1, 'expansion': 3, 'less_se_channels': False}, 'stage_idx': 3, 'block_idx': 8}\n",
      "- {'block_op': 'ir_k5_se', 'block_cfg': {'out_channels': 112, 'stride': 1, 'expansion': 3, 'less_se_channels': False}, 'stage_idx': 3, 'block_idx': 9}\n",
      "WARNING:mobile_cv.arch.utils.helper:Arguments ['width_divisor', 'dw_skip_bnrelu', 'zero_last_bn_gamma'] skipped for op Conv2d\n",
      "INFO:d2go.modeling.backbone.fbnet_v2:Build FBNet using unified arch_def:\n",
      "rpn\n",
      "- {'block_op': 'ir_k5_se', 'block_cfg': {'out_channels': 112, 'stride': 1, 'expansion': 3, 'less_se_channels': False}, 'stage_idx': 0, 'block_idx': 0}\n",
      "- {'block_op': 'ir_k5_se', 'block_cfg': {'out_channels': 112, 'stride': 1, 'expansion': 3, 'less_se_channels': False}, 'stage_idx': 0, 'block_idx': 1}\n",
      "- {'block_op': 'ir_k5_se', 'block_cfg': {'out_channels': 112, 'stride': 1, 'expansion': 3, 'less_se_channels': False}, 'stage_idx': 0, 'block_idx': 2}\n",
      "- {'block_op': 'ir_k5_se', 'block_cfg': {'out_channels': 112, 'stride': 1, 'expansion': 3, 'less_se_channels': False}, 'stage_idx': 0, 'block_idx': 3}\n",
      "- {'block_op': 'ir_k5_se', 'block_cfg': {'out_channels': 112, 'stride': 1, 'expansion': 3, 'less_se_channels': False}, 'stage_idx': 0, 'block_idx': 4}\n",
      "INFO:d2go.modeling.backbone.fbnet_v2:Build FBNet using unified arch_def:\n",
      "bbox\n",
      "- {'block_op': 'ir_k5_se', 'block_cfg': {'out_channels': 184, 'stride': 2, 'expansion': 4, 'less_se_channels': False}, 'stage_idx': 0, 'block_idx': 0}\n",
      "- {'block_op': 'ir_k3_se', 'block_cfg': {'out_channels': 184, 'stride': 1, 'expansion': 4, 'less_se_channels': False}, 'stage_idx': 0, 'block_idx': 1}\n",
      "- {'block_op': 'ir_k3_se', 'block_cfg': {'out_channels': 184, 'stride': 1, 'expansion': 4, 'less_se_channels': False}, 'stage_idx': 0, 'block_idx': 2}\n",
      "- {'block_op': 'ir_k3_se', 'block_cfg': {'out_channels': 184, 'stride': 1, 'expansion': 4, 'less_se_channels': False}, 'stage_idx': 0, 'block_idx': 3}\n",
      "- {'block_op': 'ir_k3_se', 'block_cfg': {'out_channels': 184, 'stride': 1, 'expansion': 4, 'less_se_channels': False}, 'stage_idx': 0, 'block_idx': 4}\n",
      "- {'block_op': 'ir_k5_se', 'block_cfg': {'out_channels': 200, 'stride': 1, 'expansion': 6, 'less_se_channels': False}, 'stage_idx': 0, 'block_idx': 5}\n",
      "INFO:d2go.modeling.ema:Using Model EMA.\n"
     ]
    }
   ],
   "source": [
    "model_path = \"/home/yali/coco-annotator/trained-models/model_weights_958-203_rcnn_fbnetv3a_C4.pth\"\n",
    "model = runner.build_model(cfg)\n",
    "model.load_state_dict(torch.load(model_path, map_location=cfg.MODEL.DEVICE))\n",
    "model.eval()\n",
    "\n",
    "from d2go.utils.demo_predictor import DemoPredictor\n",
    "predictor = DemoPredictor(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78c26c9b-489c-413e-a240-3f40da4ed905",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.structures import Boxes\n",
    "from detectron2.structures import Instances\n",
    "\n",
    "# Load the video\n",
    "video_path = \"1-Zenmuse_X4S_1.mp4\"  # Raw video from Zenmuse X4S cameras (IEEE FLAME Dataset)\n",
    "# video_path = \"2-Zenmuse_X4S_2.mp4\"  # Raw video from Zenmuse X4S cameras for one specific pile (IEEE FLAME Dataset)\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Detect the frames every 5 seconds\n",
    "\n",
    "# Get the frame rate of the video\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# Calculate the number of frames to skip (5 seconds * frame rate)\n",
    "skip_frames = int(5 * fps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e4f1cc6-c52a-4421-b4e5-571950e1dcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_number = 0\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break  # End of video\n",
    "\n",
    "    # Process frame every 5 seconds\n",
    "    if frame_number % skip_frames == 0:\n",
    "        # Detect fires in the frame\n",
    "        outputs = predictor(frame)\n",
    "\n",
    "        # Check if any fires are detected\n",
    "        if len(outputs[\"instances\"]) > 0:\n",
    "            # Calculate the area of the image\n",
    "            image_area = frame.shape[0] * frame.shape[1]\n",
    "            \n",
    "            height, width = frame.shape[:2]\n",
    "            \n",
    "            # Get the tensor of all bounding boxes\n",
    "            boxes_tensor = outputs[\"instances\"].pred_boxes.tensor  # Access the tensor directly\n",
    "            \n",
    "            # Filter out boxes that are more than 10% of the image size\n",
    "            filtered_indices = []\n",
    "            for i, box in enumerate(boxes_tensor):\n",
    "                x1, y1, x2, y2 = box\n",
    "                box_area = (x2 - x1) * (y2 - y1)\n",
    "                if box_area / image_area <= 0.10:  # Keep the box if it's <= 10% of the image area\n",
    "                    filtered_indices.append(i)\n",
    "                    \n",
    "            new_instances = Instances((height, width))\n",
    "            \n",
    "            # Update outputs with filtered boxes and corresponding classes and scores\n",
    "            new_instances.set(\"pred_boxes\", Boxes(boxes_tensor[filtered_indices]))\n",
    "            new_instances.set(\"pred_classes\", outputs[\"instances\"].pred_classes[filtered_indices])\n",
    "            new_instances.set(\"scores\", outputs[\"instances\"].scores[filtered_indices])\n",
    "            outputs[\"instances\"] = new_instances\n",
    "            \n",
    "            # the output object categories and corresponding bounding boxes\n",
    "            print(outputs[\"instances\"].pred_classes)\n",
    "            print(outputs[\"instances\"].pred_boxes)\n",
    "            \n",
    "            from detectron2.utils.visualizer import Visualizer\n",
    "            from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "            \n",
    "            MetadataCatalog.get(\"fire_train\").set(thing_classes=[\"fire\"])\n",
    "            MetadataCatalog.get(\"fire_val\").set(thing_classes=[\"fire\"])\n",
    "            v = Visualizer(frame[:, :, ::-1], MetadataCatalog.get(\"fire_train\"))\n",
    "            out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "            \n",
    "            # Convert visualized frame for saving\n",
    "            save_frame = out.get_image()[:, :, ::-1]\n",
    "            \n",
    "            # Save the frame with detected fires\n",
    "            frame_save_path = f\"./video_frames_1/frame_{frame_number}.jpg\"  # For video 1\n",
    "            #frame_save_path = f\"./video_frames_2/frame_{frame_number}.jpg\"  # For video 2\n",
    "            cv2.imwrite(frame_save_path, save_frame)\n",
    "\n",
    "    frame_number += 1\n",
    "\n",
    "cap.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
